{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot, Few-Shot, and Chain-of-Thought Prompting with Groq\n",
    "\n",
    "**Free Tier:** 14,400 requests/day, no credit card required!\n",
    "\n",
    "This notebook demonstrates three prompting techniques:\n",
    "1. **Zero-Shot** - No examples\n",
    "2. **Few-Shot** - With examples\n",
    "3. **Chain-of-Thought** - Step-by-step reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install groq python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "model = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "def query(prompt, temperature=0.7):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, max_tokens=500, temperature=temperature,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(f\"Ready! Using {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ZERO-SHOT: General Knowledge\")\n",
    "print(query(\"What are the benefits of regular exercise?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ZERO-SHOT: Sentiment Analysis\")\n",
    "prompt = \"\"\"Classify the sentiment as positive, negative, or neutral:\n",
    "\\\"The restaurant had amazing food but the service was incredibly slow.\\\"\n",
    "Sentiment:\"\"\"\n",
    "print(query(prompt, temperature=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FEW-SHOT: Translation\")\n",
    "prompt = \"\"\"Translate English to French:\n",
    "1. Hello. -> Bonjour.\n",
    "2. How are you? -> Comment Ã§a va?\n",
    "3. What is your name? ->\"\"\"\n",
    "print(query(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FEW-SHOT: Classification\")\n",
    "prompt = \"\"\"Classify support tickets:\n",
    "\\\"I can't log in\\\" -> Account Access\n",
    "\\\"When will my order arrive?\\\" -> Shipping\n",
    "\\\"The app keeps crashing\\\" ->\"\"\"\n",
    "print(query(prompt, temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FEW-SHOT: JSON Extraction\")\n",
    "prompt = \"\"\"Extract to JSON:\n",
    "Text: \\\"John Smith is 35, a software engineer.\\\"\n",
    "JSON: {\\\"name\\\": \\\"John Smith\\\", \\\"age\\\": 35, \\\"job\\\": \\\"software engineer\\\"}\n",
    "\n",
    "Text: \\\"CEO Robert Chen, age 52, announced the merger.\\\"\n",
    "JSON:\"\"\"\n",
    "print(query(prompt, temperature=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CHAIN-OF-THOUGHT: Math\")\n",
    "prompt = \"\"\"Solve step by step:\n",
    "A store sells notebooks for $3 and pens for $1.50.\n",
    "Sarah has $20 and buys 4 notebooks.\n",
    "How many pens can she buy with the remaining money?\"\"\"\n",
    "print(query(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CHAIN-OF-THOUGHT: Logic Puzzle\")\n",
    "prompt = \"\"\"Solve step by step:\n",
    "Alice, Bob, Carol each have a cat, dog, or fish.\n",
    "- Alice doesn't have the cat\n",
    "- Bob doesn't have the dog\n",
    "- Carol doesn't have the cat or fish\n",
    "Who has which pet?\"\"\"\n",
    "print(query(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Is 97 Prime?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Zero-Shot ---\")\n",
    "print(query(\"Is 97 a prime number? Answer yes or no.\", 0))\n",
    "\n",
    "print(\"\\n--- Few-Shot ---\")\n",
    "print(query(\"\"\"Is the number prime?\n",
    "7 -> Yes\n",
    "12 -> No\n",
    "97 ->\"\"\", 0))\n",
    "\n",
    "print(\"\\n--- Chain-of-Thought ---\")\n",
    "print(query(\"Is 97 prime? Check divisibility by 2,3,5,7 step by step.\", 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Technique | Best For |\n",
    "|-----------|----------|\n",
    "| Zero-Shot | Simple tasks |\n",
    "| Few-Shot | Custom formats |\n",
    "| Chain-of-Thought | Math, logic |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.12.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
