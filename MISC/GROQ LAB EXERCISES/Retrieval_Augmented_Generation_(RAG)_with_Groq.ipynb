{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Groq\n",
    "\n",
    "**Free Tier:** 14,400 requests/day, no credit card!\n",
    "\n",
    "RAG = Retrieve documents + Generate answers with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install groq python-dotenv faiss-cpu scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"Python is a programming language known for its simplicity.\",\n",
    "    \"The capital of France is Paris, known for the Eiffel Tower.\",\n",
    "    \"Groq provides ultra-fast LLM inference using custom hardware.\",\n",
    "    \"The Great Wall of China spans over 13,000 miles.\",\n",
    "    \"Machine learning is a subset of AI that learns from data.\",\n",
    "    \"DocuSign provides electronic signature technology.\",\n",
    "    \"RAG combines retrieval and generation for better LLM responses.\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "doc_vectors = vectorizer.fit_transform(documents).toarray()\n",
    "index = faiss.IndexFlatL2(doc_vectors.shape[1])\n",
    "index.add(doc_vectors.astype(np.float32))\n",
    "\n",
    "def retrieve(query, top_n=1):\n",
    "    vec = vectorizer.transform([query]).toarray().astype(np.float32)\n",
    "    _, indices = index.search(vec, top_n)\n",
    "    return [documents[i] for i in indices[0]]\n",
    "\n",
    "print(f\"Indexed {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "model = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "def rag_pipeline(query):\n",
    "    context = retrieve(query)[0]\n",
    "    print(f\"Retrieved: {context}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model, max_tokens=200, temperature=0.3,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Answer based only on the context provided.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {query}\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Q: What is the capital of France?\\n\")\n",
    "print(rag_pipeline(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Q: What is Python?\\n\")\n",
    "print(rag_pipeline(\"What is Python?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Q: What is Groq?\\n\")\n",
    "print(rag_pipeline(\"What is Groq?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question NOT in knowledge base\n",
    "print(\"Q: What is the population of Tokyo?\\n\")\n",
    "print(rag_pipeline(\"What is the population of Tokyo?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare: RAG vs Direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_query(q):\n",
    "    r = client.chat.completions.create(\n",
    "        model=model, max_tokens=200,\n",
    "        messages=[{\"role\": \"user\", \"content\": q}]\n",
    "    )\n",
    "    return r.choices[0].message.content\n",
    "\n",
    "q = \"What does DocuSign do?\"\n",
    "print(\"DIRECT:\", direct_query(q))\n",
    "print(\"\\nRAG:\")\n",
    "print(rag_pipeline(q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.12.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
