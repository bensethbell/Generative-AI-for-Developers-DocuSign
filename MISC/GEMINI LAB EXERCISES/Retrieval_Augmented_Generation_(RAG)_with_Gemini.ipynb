{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) with Gemini\n",
    "\n",
    "This notebook demonstrates how to build a simple RAG system using:\n",
    "- **FAISS** for fast vector similarity search\n",
    "- **TF-IDF** for document vectorization\n",
    "- **Google Gemini** for generating answers based on retrieved context\n",
    "\n",
    "**Free Tier:** No credit card required - just a Google account!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-generativeai python-dotenv faiss-cpu scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up the Knowledge Base and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents (small knowledge base)\n",
    "documents = [\n",
    "    \"Python is a programming language known for its simplicity and readability.\",\n",
    "    \"The capital of France is Paris, known for the Eiffel Tower.\",\n",
    "    \"Gemini is Google's most capable AI model, available through Google AI Studio.\",\n",
    "    \"The Great Wall of China is a historic landmark spanning over 13,000 miles.\",\n",
    "    \"Machine learning is a subset of artificial intelligence that learns from data.\",\n",
    "    \"DocuSign is a company that provides electronic signature technology.\",\n",
    "    \"RAG stands for Retrieval Augmented Generation, a technique to enhance LLM responses.\"\n",
    "]\n",
    "\n",
    "# Convert documents into TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "document_vectors = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "# Index the document vectors using FAISS\n",
    "index = faiss.IndexFlatL2(document_vectors.shape[1])\n",
    "index.add(np.array(document_vectors).astype(np.float32))\n",
    "\n",
    "print(f\"Indexed {len(documents)} documents\")\n",
    "print(f\"Vector dimension: {document_vectors.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=1):\n",
    "    \"\"\"Retrieve the most relevant documents for a query.\"\"\"\n",
    "    query_vector = vectorizer.transform([query]).toarray()\n",
    "    distances, indices = index.search(query_vector.astype(np.float32), top_n)\n",
    "    return [documents[i] for i in indices[0]]\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"What is the capital of France?\"\n",
    "retrieved = retrieve(test_query)\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Retrieved: {retrieved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set Up Gemini Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure the Gemini API\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Create model with system instruction for RAG\n",
    "model = genai.GenerativeModel(\n",
    "    \"gemini-2.0-flash\",\n",
    "    system_instruction=\"You are an AI assistant that answers questions based only on the provided context. If the context doesn't contain enough information to answer, say so.\"\n",
    ")\n",
    "\n",
    "print(f\"API key configured: {'Yes' if api_key else 'No - please set GOOGLE_API_KEY'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query, top_n=1):\n",
    "    \"\"\"\n",
    "    Run the full RAG pipeline: retrieve context and generate answer.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    retrieved_docs = retrieve(query, top_n=top_n)\n",
    "    context = \" \".join(retrieved_docs)\n",
    "    print(f\"Retrieved Document(s): {context}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Step 2: Create the prompt with context\n",
    "    user_prompt = f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Step 3: Generate answer using Gemini\n",
    "    generation_config = genai.GenerationConfig(\n",
    "        temperature=0.3,\n",
    "        max_output_tokens=200\n",
    "    )\n",
    "    \n",
    "    response = model.generate_content(\n",
    "        user_prompt,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test the RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Question with relevant context\n",
    "query1 = \"What is the capital of France?\"\n",
    "print(f\"Question: {query1}\\n\")\n",
    "response1 = rag_pipeline(query1)\n",
    "print(f\"\\nAnswer: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Question about technology\n",
    "query2 = \"What is Python?\"\n",
    "print(f\"Question: {query2}\\n\")\n",
    "response2 = rag_pipeline(query2)\n",
    "print(f\"\\nAnswer: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Question about Gemini\n",
    "query3 = \"What is Gemini?\"\n",
    "print(f\"Question: {query3}\\n\")\n",
    "response3 = rag_pipeline(query3)\n",
    "print(f\"\\nAnswer: {response3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Question NOT in knowledge base\n",
    "query4 = \"What is the population of Tokyo?\"\n",
    "print(f\"Question: {query4}\\n\")\n",
    "response4 = rag_pipeline(query4)\n",
    "print(f\"\\nAnswer: {response4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compare RAG vs Direct Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_query(query):\n",
    "    \"\"\"Ask Gemini directly without RAG context.\"\"\"\n",
    "    general_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "    response = general_model.generate_content(query)\n",
    "    return response.text\n",
    "\n",
    "# Compare responses\n",
    "test_question = \"What does DocuSign do?\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DIRECT QUERY (no RAG)\")\n",
    "print(\"=\" * 50)\n",
    "print(direct_query(test_question))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RAG QUERY (with retrieved context)\")\n",
    "print(\"=\" * 50)\n",
    "print(rag_pipeline(test_question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|----------|\n",
    "| **TF-IDF** | Converts text to numerical vectors |\n",
    "| **FAISS** | Fast similarity search for retrieval |\n",
    "| **Gemini** | Generates natural language answers |\n",
    "| **System Instruction** | Constrains answers to provided context |\n",
    "\n",
    "### API Comparison:\n",
    "\n",
    "| Feature | OpenAI | Claude | Gemini |\n",
    "|---------|--------|--------|--------|\n",
    "| Generate | `ChatCompletion.create()` | `messages.create()` | `generate_content()` |\n",
    "| System msg | In messages | `system` param | `system_instruction` |\n",
    "| Free tier | No | No | **Yes!** |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
