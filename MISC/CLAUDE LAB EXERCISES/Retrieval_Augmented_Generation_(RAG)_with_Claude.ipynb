{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) with Claude\n",
    "\n",
    "This notebook demonstrates how to build a simple RAG (Retrieval Augmented Generation) system using:\n",
    "- **FAISS** for fast vector similarity search\n",
    "- **TF-IDF** for document vectorization\n",
    "- **Claude** for generating answers based on retrieved context\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "RAG combines retrieval and generation:\n",
    "1. **Retrieve** relevant documents from a knowledge base\n",
    "2. **Augment** the prompt with retrieved context\n",
    "3. **Generate** an answer using an LLM\n",
    "\n",
    "This allows the model to answer questions using information it wasn't trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install anthropic python-dotenv faiss-cpu scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up the Knowledge Base and Retriever\n",
    "\n",
    "We'll create a simple knowledge base with sample documents and use TF-IDF + FAISS for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents (small knowledge base)\n",
    "documents = [\n",
    "    \"Python is a programming language known for its simplicity and readability.\",\n",
    "    \"The capital of France is Paris, known for the Eiffel Tower.\",\n",
    "    \"Claude is an AI assistant created by Anthropic, focused on being helpful and harmless.\",\n",
    "    \"The Great Wall of China is a historic landmark spanning over 13,000 miles.\",\n",
    "    \"Machine learning is a subset of artificial intelligence that learns from data.\",\n",
    "    \"DocuSign is a company that provides electronic signature technology.\",\n",
    "    \"RAG stands for Retrieval Augmented Generation, a technique to enhance LLM responses.\"\n",
    "]\n",
    "\n",
    "# Convert documents into TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "document_vectors = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "# Index the document vectors using FAISS (for fast retrieval)\n",
    "index = faiss.IndexFlatL2(document_vectors.shape[1])\n",
    "index.add(np.array(document_vectors).astype(np.float32))\n",
    "\n",
    "print(f\"Indexed {len(documents)} documents\")\n",
    "print(f\"Vector dimension: {document_vectors.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=1):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents for a query.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        top_n: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant documents\n",
    "    \"\"\"\n",
    "    query_vector = vectorizer.transform([query]).toarray()\n",
    "    distances, indices = index.search(query_vector.astype(np.float32), top_n)\n",
    "    return [documents[i] for i in indices[0]]\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"What is the capital of France?\"\n",
    "retrieved = retrieve(test_query)\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Retrieved: {retrieved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set Up Claude Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import anthropic\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from environment variable\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Initialize the Anthropic client\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "# Choose your Claude model\n",
    "model = \"claude-3-7-sonnet-20250219\"\n",
    "\n",
    "print(f\"Using model: {model}\")\n",
    "print(f\"API key configured: {'Yes' if api_key else 'No - please set ANTHROPIC_API_KEY'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create the RAG Pipeline\n",
    "\n",
    "The RAG pipeline:\n",
    "1. Takes a user query\n",
    "2. Retrieves relevant documents\n",
    "3. Sends the query + context to Claude\n",
    "4. Returns the generated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query, top_n=1):\n",
    "    \"\"\"\n",
    "    Run the full RAG pipeline: retrieve context and generate answer.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        top_n: Number of documents to retrieve for context\n",
    "    \n",
    "    Returns:\n",
    "        The generated answer\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    retrieved_docs = retrieve(query, top_n=top_n)\n",
    "    context = \" \".join(retrieved_docs)\n",
    "    print(f\"Retrieved Document(s): {context}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Step 2: Create the prompt with context\n",
    "    user_prompt = f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Step 3: Generate answer using Claude\n",
    "    response = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=200,\n",
    "        temperature=0.3,  # Lower temperature for factual answers\n",
    "        system=\"You are an AI assistant that answers questions based only on the provided context. If the context doesn't contain enough information to answer, say so.\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test the RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Question with relevant context in knowledge base\n",
    "query1 = \"What is the capital of France?\"\n",
    "print(f\"Question: {query1}\\n\")\n",
    "response1 = rag_pipeline(query1)\n",
    "print(f\"\\nAnswer: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Question about technology\n",
    "query2 = \"What is Python?\"\n",
    "print(f\"Question: {query2}\\n\")\n",
    "response2 = rag_pipeline(query2)\n",
    "print(f\"\\nAnswer: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Question about Claude\n",
    "query3 = \"Who created Claude?\"\n",
    "print(f\"Question: {query3}\\n\")\n",
    "response3 = rag_pipeline(query3)\n",
    "print(f\"\\nAnswer: {response3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Question NOT in knowledge base\n",
    "query4 = \"What is the population of Tokyo?\"\n",
    "print(f\"Question: {query4}\\n\")\n",
    "response4 = rag_pipeline(query4)\n",
    "print(f\"\\nAnswer: {response4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: RAG with Multiple Documents\n",
    "\n",
    "Let's retrieve multiple documents for better context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve top 2 documents for more context\n",
    "query5 = \"Tell me about AI and machine learning\"\n",
    "print(f\"Question: {query5}\\n\")\n",
    "response5 = rag_pipeline(query5, top_n=2)\n",
    "print(f\"\\nAnswer: {response5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare RAG vs Direct Query\n",
    "\n",
    "Let's see the difference between asking Claude directly vs using RAG with our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_query(query):\n",
    "    \"\"\"Ask Claude directly without RAG context.\"\"\"\n",
    "    response = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=200,\n",
    "        temperature=0.3,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "# Compare responses\n",
    "test_question = \"What does DocuSign do?\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DIRECT QUERY (no RAG)\")\n",
    "print(\"=\" * 50)\n",
    "print(direct_query(test_question))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RAG QUERY (with retrieved context)\")\n",
    "print(\"=\" * 50)\n",
    "print(rag_pipeline(test_question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we built a simple RAG system that:\n",
    "\n",
    "1. **Indexes documents** using TF-IDF vectors and FAISS\n",
    "2. **Retrieves relevant documents** based on query similarity\n",
    "3. **Generates answers** using Claude with retrieved context\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|----------|\n",
    "| **TF-IDF** | Converts text to numerical vectors |\n",
    "| **FAISS** | Fast similarity search for retrieval |\n",
    "| **Claude** | Generates natural language answers |\n",
    "| **System Prompt** | Constrains answers to provided context |\n",
    "\n",
    "### When to Use RAG:\n",
    "\n",
    "- When you have domain-specific knowledge not in the LLM's training\n",
    "- When you need up-to-date information\n",
    "- When you need answers grounded in specific documents\n",
    "- When you want to reduce hallucinations\n",
    "\n",
    "### Improvements for Production:\n",
    "\n",
    "- Use **embeddings** (e.g., from an embedding model) instead of TF-IDF for better semantic search\n",
    "- Use a **vector database** (e.g., Pinecone, Weaviate) for larger document collections\n",
    "- Implement **chunking** for long documents\n",
    "- Add **reranking** to improve retrieval quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
