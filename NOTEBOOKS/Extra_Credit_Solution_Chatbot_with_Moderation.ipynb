{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit Solution — Interactive Chatbot with Moderation\n",
    "\n",
    "This notebook provides solutions for the three extra credit exercises from **Lab Exercise 2**:\n",
    "\n",
    "1. **Part 1** — Build an interactive chatbot using Groq\n",
    "2. **Part 2** — Add conversation history so the chatbot remembers context\n",
    "3. **Part 3** — Add content moderation using meta-prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install groq python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Configure Groq client\n",
    "client = Groq(api_key=api_key)\n",
    "model = \"llama-3.3-70b-versatile\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Interactive Chatbot\n",
    "\n",
    "A simple loop that prompts the user for input, sends it to the Groq API, and prints the response. Type `quit` or `exit` to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chatbot ready! Type 'quit' or 'exit' to stop.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.strip().lower() in [\"quit\", \"exit\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=256,\n",
    "    )\n",
    "\n",
    "    reply = response.choices[0].message.content\n",
    "    print(f\"Assistant: {reply}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Add Conversation History\n",
    "\n",
    "The chatbot above has no memory — each turn is independent. To give the model context of the full conversation, we maintain a `messages` list and append both the **user** message and the **assistant** reply after every exchange.\n",
    "\n",
    "The `\"assistant\"` role represents the model's own previous replies. By including these in the messages list, the model can \"remember\" what it said earlier and maintain coherent, multi-turn conversations.\n",
    "\n",
    "**Test it:** Tell the chatbot your name, then in a later turn ask *\"What is my name?\"* — it should remember."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chatbot with memory ready! Type 'quit' or 'exit' to stop.\\n\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.strip().lower() in [\"quit\", \"exit\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Append the user's message to the conversation history\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=256,\n",
    "    )\n",
    "\n",
    "    reply = response.choices[0].message.content\n",
    "\n",
    "    # Append the assistant's reply to the conversation history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "    print(f\"Assistant: {reply}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Add Content Moderation\n",
    "\n",
    "We use **meta-prompting** to make the model judge whether a piece of text contains harmful content (e.g., racism, hate speech, illegal requests, or other inappropriate material).\n",
    "\n",
    "The `moderate` function sends the text to the model with a system prompt that instructs it to respond with only `\"yes\"` (harmful) or `\"no\"` (safe). We parse the response into a boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moderate(text):\n",
    "    \"\"\"Return True if the text is flagged as harmful, False otherwise.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a content moderation assistant. \"\n",
    "                    \"Determine whether the following text contains harmful content \"\n",
    "                    \"such as racism, hate speech, sexually explicit material, \"\n",
    "                    \"encouragement of violence, illegal activity requests, \"\n",
    "                    \"or other inappropriate material. \"\n",
    "                    \"Respond with ONLY 'yes' or 'no'. \"\n",
    "                    \"'yes' means the content IS harmful. 'no' means it is safe.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=4,\n",
    "    )\n",
    "    result = response.choices[0].message.content.strip().lower()\n",
    "    return result.startswith(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAFE_FALLBACK = \"I'm sorry, I can't respond to that. Let's keep our conversation respectful and safe.\"\n",
    "\n",
    "print(\"Chatbot with memory + moderation ready! Type 'quit' or 'exit' to stop.\\n\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.strip().lower() in [\"quit\", \"exit\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Moderate the user's input\n",
    "    if moderate(user_input):\n",
    "        print(f\"Assistant: {SAFE_FALLBACK}\\n\")\n",
    "        continue\n",
    "\n",
    "    # Append user message and get response\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=256,\n",
    "    )\n",
    "\n",
    "    reply = response.choices[0].message.content\n",
    "\n",
    "    # Moderate the assistant's response\n",
    "    if moderate(reply):\n",
    "        reply = SAFE_FALLBACK\n",
    "\n",
    "    # Append assistant reply to history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "    print(f\"Assistant: {reply}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
