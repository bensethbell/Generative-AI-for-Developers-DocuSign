{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with Groq\n",
    "\n",
    "This notebook demonstrates how to build a simple chatbot using Groq's API.\n",
    "\n",
    "## Setup Instructions:\n",
    "\n",
    "1. **Get a Groq API key:**\n",
    "   - Go to https://console.groq.com/\n",
    "   - Sign up or log in\n",
    "   - Go to API Keys section\n",
    "   - Create a new API key\n",
    "\n",
    "2. **Set your API key in Cell 2**\n",
    "\n",
    "3. **Run the cells in order!**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from groq) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install groq python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Groq client initialized!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client\n",
    "load_dotenv()\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "print(\"‚úÖ Groq client initialized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Chatbot Function\n",
    "\n",
    "This chatbot is designed to be an expert baker assistant that helps with sourdough bread questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chatbot function defined!\n"
     ]
    }
   ],
   "source": [
    "def chat_with_bot(user_input, model=\"llama-3.3-70b-versatile\"):\n",
    "    \"\"\"\n",
    "    Send a message to the chatbot and get a response.\n",
    "\n",
    "    Args:\n",
    "        user_input: The user's message\n",
    "        model: Groq model to use (default: llama-3.3-70b-versatile)\n",
    "\n",
    "    Returns:\n",
    "        str: The chatbot's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are an expert baker assistant that helps home bakers with questions about sourdough bread dough.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": user_input\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Chatbot function defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Chatbot\n",
    "\n",
    "Let's try asking the chatbot some questions about sourdough bread!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: When is starter ready to use?\n",
      "\n",
      "Chatbot: Your sourdough starter is the heart of your bread-making process. It's ready to use when it has reached a state of maturity and stability, which usually takes around 7-14 days of regular feeding and care. Here are some signs to look out for to determine if your starter is ready to use:\n",
      "\n",
      "1. **Active Bubbles**: A healthy, active starter will have plenty of bubbles on its surface and throughout the mixture. This indicates that the wild yeast and bacteria are fermenting the sugars and producing carbon dioxide.\n",
      "2. **Doubled in Size**: A mature starter will roughly double in size within 4-6 hours after feeding. This is a sign that the microorganisms are actively consuming the sugars and producing CO2.\n",
      "3. **Tangy Aroma**: A sourdough starter will have a tangy, slightly sour smell, which is a result of the lactic acid produced by the bacteria.\n",
      "4. **Consistent Behavior**: A mature starter will have a consistent behavior, meaning it will respond predictably to feedings and environment changes. It will also have a consistent texture, which is usually thick and creamy, similar to pancake batter.\n",
      "5. **pH Level**: A mature starter will have a pH level between 3.5 and 4.5, which is slightly acidic.\n",
      "\n",
      "To test if your starter is ready to use, you can try the following:\n",
      "\n",
      "* **Float Test**: Drop a small amount of starter into a glass of water. If it floats, it's ready to use. If it sinks, it needs more time to mature.\n",
      "* **Bread Test**: Mix a small amount of starter with flour and water to create a mini-dough. If it rises and has a nice texture, your starter is likely ready to use.\n",
      "\n",
      "Remember, every starter is unique, and the time it takes to mature can vary depending on factors like temperature, feeding schedule, and the type of flour used. Be patient, and with regular care and feeding, your starter will be ready to help you create delicious sourdough bread in no time!\n"
     ]
    }
   ],
   "source": [
    "# Test question 1\n",
    "user_input = \"When is starter ready to use?\"\n",
    "response = chat_with_bot(user_input)\n",
    "print(f\"You: {user_input}\")\n",
    "print(f\"\\nChatbot: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Chat Loop\n",
    "\n",
    "Run this cell to have an interactive conversation with the chatbot. Type 'exit' or 'quit' to end the conversation.\n",
    "\n",
    "**Note:** This will only work in a regular Jupyter notebook, not in JupyterLab or some cloud environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chat loop\n",
    "print(\"Chatbot: Hello! I'm your sourdough bread assistant. Ask me anything about sourdough baking!\")\n",
    "print(\"(Type 'exit' or 'quit' to end the conversation)\\n\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"\\nChatbot: Happy baking!\")\n",
    "            break\n",
    "        \n",
    "        response = chat_with_bot(user_input)\n",
    "        print(f\"\\nChatbot: {response}\\n\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nChatbot: Happy baking!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Conversation History\n",
    "\n",
    "The above chatbot doesn't remember previous messages. Here's an improved version that maintains conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    \"\"\"\n",
    "    A chatbot with conversation memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, system_prompt=None, model=\"llama-3.3-70b-versatile\"):\n",
    "        self.model = model\n",
    "        self.conversation_history = []\n",
    "\n",
    "        # Set system prompt\n",
    "        if system_prompt is None:\n",
    "            system_prompt = \"You are an expert baker assistant that helps home bakers with questions about sourdough bread dough.\"\n",
    "\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        })\n",
    "\n",
    "    def chat(self, user_message):\n",
    "        \"\"\"\n",
    "        Send a message and get a response while maintaining conversation history.\n",
    "        \"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        })\n",
    "\n",
    "        try:\n",
    "            # Get response from Groq\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=self.conversation_history,\n",
    "                temperature=0.7,\n",
    "                max_tokens=500\n",
    "            )\n",
    "\n",
    "            # Extract assistant's message\n",
    "            assistant_message = response.choices[0].message.content\n",
    "\n",
    "            # Add assistant's response to history\n",
    "            self.conversation_history.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": assistant_message\n",
    "            })\n",
    "\n",
    "            return assistant_message\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset conversation history.\"\"\"\n",
    "        system_message = self.conversation_history[0]\n",
    "        self.conversation_history = [system_message]\n",
    "        print(\"Conversation history reset!\")\n",
    "\n",
    "print(\"‚úÖ Chatbot class defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chatbot with memory\n",
    "bot = Chatbot()\n",
    "\n",
    "# First question\n",
    "response1 = bot.chat(\"What's the ideal temperature for proofing sourdough?\")\n",
    "print(\"You: What's the ideal temperature for proofing sourdough?\")\n",
    "print(f\"Bot: {response1}\\n\")\n",
    "\n",
    "# Follow-up question (references previous context)\n",
    "response2 = bot.chat(\"What if my kitchen is colder than that?\")\n",
    "print(\"You: What if my kitchen is colder than that?\")\n",
    "print(f\"Bot: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see the full conversation history\n",
    "print(\"Full conversation history:\")\n",
    "for msg in bot.conversation_history:\n",
    "    role = msg['role'].capitalize()\n",
    "    content = msg['content'][:100] + \"...\" if len(msg['content']) > 100 else msg['content']\n",
    "    print(f\"\\n{role}: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate Meta Prompting\n",
    "\n",
    "Ask your model to become its own content moderator!\n",
    "\n",
    "**Meta prompting** is a technique where you use the LLM to evaluate and moderate its own inputs and outputs. This creates a safety layer that can:\n",
    "\n",
    "- Filter harmful user inputs before processing\n",
    "- Check generated responses before returning them to users\n",
    "- Enforce content policies without hard-coded rules\n",
    "\n",
    "### The Challenge:\n",
    "\n",
    "1. Design a prompt that asks your model to identify whether or not a prompt or response contains harmful content.\n",
    "   * Define a specific kind of \"harmful content\":\n",
    "     * e.g. \"copyright infringement\"\n",
    "     * e.g. \"racist language\"\n",
    "     * e.g. \"mature themes not appropriate for children\"\n",
    "\n",
    "2. Before making any prompts and after generating any responses:\n",
    "   * Inject the prompt or content to your meta prompt\n",
    "   * Send that to the model\n",
    "   * Parse the \"yes\" or \"no\" into a boolean\n",
    "   * Reject the prompt or response if the model said you should"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Content Moderation Function\n",
    "\n",
    "First, let's create a function that checks if content contains harmful material:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_content_safety(content, content_type=\"message\", model=\"llama-3.1-8b-instant\"):\n",
    "    \"\"\"\n",
    "    Use the LLM to check if content contains harmful material.\n",
    "    \n",
    "    Args:\n",
    "        content: The text to check (user input or bot response)\n",
    "        content_type: Either \"user input\" or \"bot response\" for context\n",
    "        model: Model to use for moderation (smaller/faster model recommended)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (is_safe: bool, reason: str)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define what types of harmful content to check for\n",
    "    # You can customize this list for your specific use case\n",
    "    meta_prompt = f\"\"\"You are a content moderation system. Your job is to determine if the following {content_type} contains harmful content.\n",
    "\n",
    "Check for the following types of harmful content:\n",
    "1. Hate speech or discriminatory language (racism, sexism, etc.)\n",
    "2. Explicit violence or threats\n",
    "3. Adult/sexual content not appropriate for general audiences\n",
    "4. Instructions for illegal activities\n",
    "5. Personal attacks or harassment\n",
    "\n",
    "Content to evaluate:\n",
    "\\\"\\\"\\\"\n",
    "{content}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Respond with ONLY one of these two options:\n",
    "- \"SAFE\" if the content is acceptable\n",
    "- \"UNSAFE: [brief reason]\" if the content is harmful\n",
    "\n",
    "Your response:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a content moderation assistant. Be strict but fair.\"},\n",
    "                {\"role\": \"user\", \"content\": meta_prompt}\n",
    "            ],\n",
    "            temperature=0.1,  # Low temperature for consistent moderation\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content.strip().upper()\n",
    "        \n",
    "        if result.startswith(\"SAFE\"):\n",
    "            return True, \"Content is safe\"\n",
    "        elif result.startswith(\"UNSAFE\"):\n",
    "            reason = result.replace(\"UNSAFE:\", \"\").replace(\"UNSAFE\", \"\").strip()\n",
    "            return False, reason if reason else \"Content flagged as potentially harmful\"\n",
    "        else:\n",
    "            # If unclear, err on the side of caution\n",
    "            return True, \"Content appears safe\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        # If moderation fails, log error but allow content (fail open)\n",
    "        # In production, you might want to fail closed instead\n",
    "        print(f\"Moderation error: {e}\")\n",
    "        return True, f\"Moderation check failed: {e}\"\n",
    "\n",
    "print(\"‚úÖ Content moderation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the moderation function with safe content\n",
    "safe_message = \"How long should I proof my sourdough bread?\"\n",
    "is_safe, reason = check_content_safety(safe_message, \"user input\")\n",
    "print(f\"Message: '{safe_message}'\")\n",
    "print(f\"Is Safe: {is_safe}\")\n",
    "print(f\"Reason: {reason}\")\n",
    "print()\n",
    "\n",
    "# Test with potentially problematic content\n",
    "unsafe_message = \"I hate everyone who doesn't like sourdough bread, they should all suffer!\"\n",
    "is_safe, reason = check_content_safety(unsafe_message, \"user input\")\n",
    "print(f\"Message: '{unsafe_message}'\")\n",
    "print(f\"Is Safe: {is_safe}\")\n",
    "print(f\"Reason: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moderated Chatbot Class\n",
    "\n",
    "Now let's create a chatbot that automatically moderates both user inputs AND its own responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModeratedChatbot:\n",
    "    \"\"\"\n",
    "    A chatbot with conversation memory AND content moderation.\n",
    "    Uses meta-prompting to check both user inputs and bot responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt=None, model=\"llama-3.3-70b-versatile\", \n",
    "                 moderation_model=\"llama-3.1-8b-instant\"):\n",
    "        self.model = model\n",
    "        self.moderation_model = moderation_model\n",
    "        self.conversation_history = []\n",
    "        self.moderation_log = []  # Track moderation decisions\n",
    "        \n",
    "        # Set system prompt\n",
    "        if system_prompt is None:\n",
    "            system_prompt = \"You are an expert baker assistant that helps home bakers with questions about sourdough bread.\"\n",
    "        \n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        })\n",
    "    \n",
    "    def _moderate_content(self, content, content_type):\n",
    "        \"\"\"Internal method to check content safety.\"\"\"\n",
    "        is_safe, reason = check_content_safety(\n",
    "            content, \n",
    "            content_type, \n",
    "            model=self.moderation_model\n",
    "        )\n",
    "        \n",
    "        # Log the moderation decision\n",
    "        self.moderation_log.append({\n",
    "            \"content_type\": content_type,\n",
    "            \"content_preview\": content[:100] + \"...\" if len(content) > 100 else content,\n",
    "            \"is_safe\": is_safe,\n",
    "            \"reason\": reason\n",
    "        })\n",
    "        \n",
    "        return is_safe, reason\n",
    "    \n",
    "    def chat(self, user_message):\n",
    "        \"\"\"\n",
    "        Send a message and get a response with moderation on both ends.\n",
    "        \"\"\"\n",
    "        # STEP 1: Moderate the user's input BEFORE processing\n",
    "        input_safe, input_reason = self._moderate_content(user_message, \"user input\")\n",
    "        \n",
    "        if not input_safe:\n",
    "            return f\"‚ö†Ô∏è I can't process that message. Reason: {input_reason}\"\n",
    "        \n",
    "        # STEP 2: Add user message to history and get response\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=self.conversation_history,\n",
    "                temperature=0.7,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            \n",
    "            assistant_message = response.choices[0].message.content\n",
    "            \n",
    "            # STEP 3: Moderate the bot's response BEFORE returning it\n",
    "            output_safe, output_reason = self._moderate_content(assistant_message, \"bot response\")\n",
    "            \n",
    "            if not output_safe:\n",
    "                # Remove the user message from history since we're not completing this exchange\n",
    "                self.conversation_history.pop()\n",
    "                return f\"‚ö†Ô∏è I generated a response but it was flagged by moderation. Please rephrase your question.\"\n",
    "            \n",
    "            # STEP 4: If response is safe, add to history and return\n",
    "            self.conversation_history.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": assistant_message\n",
    "            })\n",
    "            \n",
    "            return assistant_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.conversation_history.pop()  # Remove failed user message\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def get_moderation_log(self):\n",
    "        \"\"\"Return the moderation log for inspection.\"\"\"\n",
    "        return self.moderation_log\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset conversation history and moderation log.\"\"\"\n",
    "        system_message = self.conversation_history[0]\n",
    "        self.conversation_history = [system_message]\n",
    "        self.moderation_log = []\n",
    "        print(\"Conversation and moderation log reset!\")\n",
    "\n",
    "print(\"‚úÖ ModeratedChatbot class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Moderated Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a moderated chatbot\n",
    "moderated_bot = ModeratedChatbot()\n",
    "\n",
    "# Test with a safe question\n",
    "print(\"Test 1: Safe question\")\n",
    "print(\"-\" * 40)\n",
    "response = moderated_bot.chat(\"What's the best flour for sourdough?\")\n",
    "print(f\"User: What's the best flour for sourdough?\")\n",
    "print(f\"Bot: {response[:300]}...\")  # Truncate for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a potentially problematic message\n",
    "print(\"\\nTest 2: Potentially problematic input\")\n",
    "print(\"-\" * 40)\n",
    "response = moderated_bot.chat(\"I hate people who use bread machines, they're all idiots who deserve bad bread!\")\n",
    "print(f\"User: I hate people who use bread machines, they're all idiots who deserve bad bread!\")\n",
    "print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the moderation log to see what was checked\n",
    "print(\"Moderation Log:\")\n",
    "print(\"=\" * 50)\n",
    "for i, entry in enumerate(moderated_bot.get_moderation_log()):\n",
    "    print(f\"\\n[{i+1}] Type: {entry['content_type']}\")\n",
    "    print(f\"    Preview: {entry['content_preview']}\")\n",
    "    print(f\"    Safe: {entry['is_safe']}\")\n",
    "    print(f\"    Reason: {entry['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Moderated Chat\n",
    "\n",
    "Run this cell for an interactive chat with content moderation enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive moderated chat loop\n",
    "# Uncomment and run to start chatting\n",
    "\n",
    "# moderated_bot.reset()  # Start fresh\n",
    "# \n",
    "# print(\"üõ°Ô∏è Moderated Chatbot\")\n",
    "# print(\"Content moderation is enabled for both inputs and outputs.\")\n",
    "# print(\"Type 'exit' to quit, 'log' to see moderation history\")\n",
    "# print(\"-\" * 50)\n",
    "# \n",
    "# while True:\n",
    "#     try:\n",
    "#         user_input = input(\"\\nYou: \")\n",
    "#         \n",
    "#         if user_input.lower() == \"exit\":\n",
    "#             print(\"Goodbye!\")\n",
    "#             break\n",
    "#         elif user_input.lower() == \"log\":\n",
    "#             for entry in moderated_bot.get_moderation_log()[-5:]:  # Last 5 entries\n",
    "#                 print(f\"  [{entry['content_type']}] Safe={entry['is_safe']}: {entry['content_preview'][:50]}...\")\n",
    "#             continue\n",
    "#         \n",
    "#         response = moderated_bot.chat(user_input)\n",
    "#         print(f\"\\nBot: {response}\")\n",
    "#         \n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"\\n\\nGoodbye!\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways: Meta Prompting for Content Moderation\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. **Meta prompting** uses the LLM to evaluate content before/after processing\n",
    "2. **Two-way moderation**: Check both user inputs AND bot responses\n",
    "3. **Use a faster model** (like `llama-3.1-8b-instant`) for moderation to reduce latency\n",
    "4. **Keep a log** of moderation decisions for debugging and auditing\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "| Practice | Why |\n",
    "|----------|-----|\n",
    "| Define specific harmful content types | More accurate than generic \"harmful\" |\n",
    "| Use low temperature (0.1) for moderation | Consistent, predictable results |\n",
    "| Log all moderation decisions | Debugging and compliance |\n",
    "| Fail gracefully | Don't crash if moderation fails |\n",
    "\n",
    "**Customization Ideas:**\n",
    "\n",
    "- Add domain-specific moderation rules (e.g., medical misinformation)\n",
    "- Implement severity levels (warn vs. block)\n",
    "- Add human review queue for borderline cases\n",
    "- Create allow-lists for specific terms that might be falsely flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
