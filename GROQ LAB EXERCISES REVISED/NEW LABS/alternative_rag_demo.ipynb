{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System for Analyzing PDF Documents with Groq\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system using:\n",
    "- **Groq** for fast LLM inference (Llama models)\n",
    "- **FAISS** for vector storage and similarity search\n",
    "- **LangChain** for orchestrating the RAG pipeline\n",
    "\n",
    "## Why Groq?\n",
    "- **Speed**: Groq's LPU (Language Processing Unit) provides extremely fast inference\n",
    "- **Free tier**: Generous free API access for development\n",
    "- **Open models**: Access to Llama 3.3, Mixtral, and other open-source models\n",
    "- **Simple API**: No need for local GPU resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### 1. Get a Groq API Key\n",
    "1. Go to https://console.groq.com\n",
    "2. Sign up for a free account\n",
    "3. Create an API key\n",
    "4. Set it in Cell 3 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-groq langchain-community\n",
    "!pip install faiss-cpu\n",
    "!pip install pypdf\n",
    "!pip install sentence-transformers\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Set your Groq API key here (or use environment variable)\n",
    "os.environ[\"GROQ_API_KEY\"] = \"your-groq-api-key-here\"  # Replace with your actual key\n",
    "\n",
    "# Verify the key is set\n",
    "if os.environ.get(\"GROQ_API_KEY\") == \"your-groq-api-key-here\":\n",
    "    print(\"WARNING: Please set your actual Groq API key!\")\n",
    "else:\n",
    "    print(\"Groq API key is set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Process the PDF Document\n",
    "\n",
    "We'll load a PDF and split it into chunks for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your PDF document\n",
    "# Replace with your PDF file path\n",
    "pdf_file_path = \"your_document.pdf\"\n",
    "\n",
    "# For demo purposes, let's create a simple text-based example if no PDF is available\n",
    "try:\n",
    "    loader = PyPDFLoader(pdf_file_path)\n",
    "    pages = loader.load_and_split()\n",
    "    print(f\"Loaded {len(pages)} pages from PDF\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"PDF not found at {pdf_file_path}\")\n",
    "    print(\"Creating sample documents for demonstration...\")\n",
    "    \n",
    "    # Create sample documents for demo\n",
    "    from langchain.schema import Document\n",
    "    pages = [\n",
    "        Document(\n",
    "            page_content=\"\"\"Task Decomposition is a technique where complex tasks are broken down \n",
    "            into smaller, more manageable sub-tasks. This approach is commonly used in AI systems \n",
    "            to improve problem-solving capabilities. Methods include Chain of Thought (CoT), \n",
    "            Tree of Thoughts (ToT), and Plan-and-Execute strategies.\"\"\",\n",
    "            metadata={\"source\": \"sample\", \"page\": 0}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"\"\"Multi-agent systems consist of multiple AI agents working together \n",
    "            to accomplish tasks. Each agent can have specialized roles and capabilities. \n",
    "            These systems excel at tasks requiring collaboration, debate, and diverse perspectives. \n",
    "            Applications include software development, research, and complex problem-solving.\"\"\",\n",
    "            metadata={\"source\": \"sample\", \"page\": 1}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"\"\"Retrieval-Augmented Generation (RAG) combines the power of large \n",
    "            language models with external knowledge retrieval. The system first retrieves \n",
    "            relevant documents from a knowledge base, then uses this context to generate \n",
    "            accurate and informed responses. This reduces hallucinations and improves factual accuracy.\"\"\",\n",
    "            metadata={\"source\": \"sample\", \"page\": 2}\n",
    "        )\n",
    "    ]\n",
    "    print(f\"Created {len(pages)} sample documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first document\n",
    "print(\"First document content:\")\n",
    "print(pages[0].page_content[:500])\n",
    "print(\"\\nMetadata:\", pages[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the Vector Store\n",
    "\n",
    "We'll use HuggingFace embeddings (free, runs locally) and FAISS for the vector store.\n",
    "\n",
    "### Why these choices?\n",
    "- **HuggingFace Embeddings**: Free, no API key needed, runs locally\n",
    "- **FAISS**: Fast, efficient vector similarity search by Meta/Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model (free, runs locally)\n",
    "# This model is good for general-purpose text embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",  # Small, fast, and effective\n",
    "    model_kwargs={'device': 'cpu'}   # Use 'cuda' if you have a GPU\n",
    ")\n",
    "\n",
    "print(\"Embedding model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FAISS vector store from our documents\n",
    "vectorstore = FAISS.from_documents(pages, embedding=embeddings)\n",
    "\n",
    "print(f\"Vector store created with {len(pages)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the vector store with a similarity search\n",
    "query = \"What is task decomposition?\"\n",
    "docs = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 2 most similar documents:\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize the Groq LLM\n",
    "\n",
    "Groq provides access to several powerful open-source models:\n",
    "\n",
    "| Model | Description | Best For |\n",
    "|-------|-------------|----------|\n",
    "| `llama-3.3-70b-versatile` | Latest Llama, very capable | Complex reasoning |\n",
    "| `llama-3.1-8b-instant` | Smaller, faster Llama | Quick responses |\n",
    "| `mixtral-8x7b-32768` | Mixture of experts | Long context tasks |\n",
    "| `gemma2-9b-it` | Google's Gemma 2 | Balanced performance |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Groq LLM\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",  # You can change this to other models\n",
    "    temperature=0.2,  # Lower = more focused/deterministic\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"Groq LLM initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of the LLM\n",
    "response = llm.invoke(\"Say hello in one sentence.\")\n",
    "print(\"LLM Test Response:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build the Basic RAG Chain\n",
    "\n",
    "Now we'll combine the vector store and LLM into a RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the RAG chain\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(vectorstore.as_retriever(), question_answer_chain)\n",
    "\n",
    "print(\"RAG chain created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG chain\n",
    "response = rag_chain.invoke({\"input\": \"What is task decomposition?\"})\n",
    "\n",
    "print(\"Question:\", response['input'])\n",
    "print(\"\\nAnswer:\", response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the retrieved context\n",
    "print(\"Retrieved Context Documents:\")\n",
    "for i, doc in enumerate(response['context']):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(doc.page_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Add Chat History (Conversational RAG)\n",
    "\n",
    "To make the system conversational, we need to:\n",
    "1. Track chat history\n",
    "2. Reformulate questions based on context\n",
    "3. Use history-aware retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a history-aware retriever\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, vectorstore.as_retriever(), contextualize_q_prompt\n",
    ")\n",
    "\n",
    "print(\"History-aware retriever created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the conversational RAG chain\n",
    "qa_system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "conversational_rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "print(\"Conversational RAG chain created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a conversation!\n",
    "chat_history = []\n",
    "\n",
    "# First question\n",
    "question1 = \"What is Task Decomposition?\"\n",
    "response1 = conversational_rag_chain.invoke({\n",
    "    \"input\": question1, \n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "print(f\"Q: {question1}\")\n",
    "print(f\"A: {response1['answer']}\\n\")\n",
    "\n",
    "# Update chat history\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question1),\n",
    "    AIMessage(content=response1[\"answer\"]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up question (uses \"it\" to refer to previous topic)\n",
    "question2 = \"What are common ways of doing it?\"\n",
    "response2 = conversational_rag_chain.invoke({\n",
    "    \"input\": question2, \n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "print(f\"Q: {question2}\")\n",
    "print(f\"A: {response2['answer']}\")\n",
    "\n",
    "# The system understands \"it\" refers to \"Task Decomposition\" from the previous question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the full chat history\n",
    "print(\"Chat History:\")\n",
    "for msg in chat_history:\n",
    "    role = \"Human\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "    print(f\"\\n{role}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Interactive Chat Loop\n",
    "\n",
    "Let's create an interactive chat interface!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_documents(rag_chain, initial_history=None):\n",
    "    \"\"\"\n",
    "    Interactive chat function for RAG system.\n",
    "    Type 'quit' or 'exit' to stop.\n",
    "    Type 'clear' to reset chat history.\n",
    "    \"\"\"\n",
    "    chat_history = initial_history if initial_history else []\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"RAG Chat with Groq\")\n",
    "    print(\"Commands: 'quit'/'exit' to stop, 'clear' to reset\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        elif user_input.lower() == 'clear':\n",
    "            chat_history = []\n",
    "            print(\"Chat history cleared.\")\n",
    "            continue\n",
    "        elif not user_input:\n",
    "            continue\n",
    "            \n",
    "        # Get response from RAG chain\n",
    "        response = rag_chain.invoke({\n",
    "            \"input\": user_input,\n",
    "            \"chat_history\": chat_history\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nAssistant: {response['answer']}\")\n",
    "        \n",
    "        # Update history\n",
    "        chat_history.extend([\n",
    "            HumanMessage(content=user_input),\n",
    "            AIMessage(content=response[\"answer\"]),\n",
    "        ])\n",
    "    \n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run interactive chat\n",
    "# history = chat_with_documents(conversational_rag_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we built a complete RAG system using:\n",
    "\n",
    "1. **Document Loading**: PyPDFLoader for PDF processing\n",
    "2. **Embeddings**: HuggingFace's `all-MiniLM-L6-v2` (free, local)\n",
    "3. **Vector Store**: FAISS for efficient similarity search\n",
    "4. **LLM**: Groq's `llama-3.3-70b-versatile` (fast, free tier available)\n",
    "5. **Orchestration**: LangChain for RAG pipeline\n",
    "\n",
    "### Key Advantages of This Setup:\n",
    "- **Cost-effective**: Free embeddings + Groq's generous free tier\n",
    "- **Fast**: Groq's LPU provides extremely fast inference\n",
    "- **Simple**: No local GPU needed for the LLM\n",
    "- **Powerful**: Access to state-of-the-art Llama models\n",
    "\n",
    "### Next Steps:\n",
    "- Try different Groq models for different use cases\n",
    "- Experiment with chunk sizes and overlap\n",
    "- Add more documents to your knowledge base\n",
    "- Implement persistent vector storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Save and load the vector store for later use\n",
    "\n",
    "# Save\n",
    "# vectorstore.save_local(\"my_vectorstore\")\n",
    "\n",
    "# Load\n",
    "# loaded_vectorstore = FAISS.load_local(\n",
    "#     \"my_vectorstore\", \n",
    "#     embeddings,\n",
    "#     allow_dangerous_deserialization=True\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
