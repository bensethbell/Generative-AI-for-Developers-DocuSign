{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) with Groq\n",
    "\n",
    "**Converted to use Groq API**\n",
    "\n",
    "## Setup Instructions:\n",
    "\n",
    "1. **Get a Groq API key:**\n",
    "   - Go to https://console.groq.com/\n",
    "   - Sign up or log in\n",
    "   - Go to API Keys section\n",
    "   - Create a new API key\n",
    "\n",
    "2. **Set your API key in Cell 3**\n",
    "\n",
    "3. **Run the cells in order!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install groq python-dotenv faiss-cpu scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents (small knowledge base)\n",
    "documents = [\n",
    "    \"Python is a programming language.\",\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"Groq is a cloud computing platform by Microsoft.\",\n",
    "    \"Groq provides powerful language models for various tasks.\",\n",
    "    \"The Great Wall of China is a historic landmark.\"\n",
    "]\n",
    "\n",
    "# Convert documents into TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "document_vectors = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "# Create a FAISS index for fast similarity search\n",
    "dimension = document_vectors.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(document_vectors.astype(np.float32))\n",
    "\n",
    "print(f\"Indexed {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Set your Groq API key\n",
    "os.environ[\"GROQ_API_KEY\"] = \"gsk_your_api_key_here\"  # Replace with your actual key\n",
    "\n",
    "# Initialize Groq client\n",
    "load_dotenv()\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the most relevant document\n",
    "def retrieve(query, top_n=1):\n",
    "    query_vector = vectorizer.transform([query]).toarray()\n",
    "    distances, indices = index.search(query_vector.astype(np.float32), top_n)\n",
    "    return [documents[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query, model=\"llama-3.3-70b-versatile\"):\n",
    "    \"\"\"\n",
    "    RAG pipeline: Retrieve relevant documents and generate answer\n",
    "\n",
    "    Args:\n",
    "        query: The user's question\n",
    "        model: Groq model to use\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve the most relevant document\n",
    "    retrieved_docs = retrieve(query, top_n=1)\n",
    "    context = \" \".join(retrieved_docs)\n",
    "    print(\"Retrieved Document:\", context)\n",
    "\n",
    "    # Step 2: Use Groq to generate an answer using the retrieved document\n",
    "    prompt = f\"Based on the following context, answer the question: {query}\n",
    "\n",
    "Context: {context}\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information. Only provide answers based on the context provided.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=200\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG pipeline\n",
    "response = rag_pipeline(\"What is the capital of France?\")\n",
    "print(\"\\nFinal Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another question\n",
    "response2 = rag_pipeline(\"What is Python?\")\n",
    "print(\"\\nFinal Response:\", response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a question about something not in the knowledge base\n",
    "response3 = rag_pipeline(\"What is machine learning?\")\n",
    "print(\"\\nFinal Response:\", response3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
