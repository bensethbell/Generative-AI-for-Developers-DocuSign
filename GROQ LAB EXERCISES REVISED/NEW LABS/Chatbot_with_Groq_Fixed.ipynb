{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with Groq\n",
    "\n",
    "**Converted to use Groq API (from Groq)**\n",
    "\n",
    "This notebook demonstrates how to build a simple chatbot using Groq's API.\n",
    "\n",
    "## Setup Instructions:\n",
    "\n",
    "1. **Get a Groq API key:**\n",
    "   - Go to https://console.groq.com/\n",
    "   - Sign up or log in\n",
    "   - Go to API Keys section\n",
    "   - Create a new API key\n",
    "   - Add $5-10 in credits to your account\n",
    "\n",
    "2. **Set your API key in Cell 2**\n",
    "\n",
    "3. **Run the cells in order!**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install groq python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Set your Groq API key\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"  # Replace with your actual key\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "print(\"✅ Groq client initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Chatbot Function\n",
    "\n",
    "This chatbot is designed to be an expert baker assistant that helps with sourdough bread questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_bot(user_input, model=\"llama-3.3-70b-versatile\"):\n",
    "    \"\"\"\n",
    "    Send a message to the chatbot and get a response.\n",
    "    \n",
    "    Args:\n",
    "        user_input: The user's message\n",
    "        model: Groq model to use (default: llama-3.3-70b-versatile)\n",
    "    \n",
    "    Returns:\n",
    "        str: The chatbot's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are an expert baker assistant that helps home bakers with questions about sourdough bread dough.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": user_input\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"✅ Chatbot function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Chatbot\n",
    "\n",
    "Let's try asking the chatbot some questions about sourdough bread!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 1\n",
    "user_input = \"When is starter ready to use?\"\n",
    "response = chat_with_bot(user_input)\n",
    "print(f\"You: {user_input}\")\n",
    "print(f\"\\nChatbot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 2\n",
    "user_input = \"How do I know if my dough is overproofed?\"\n",
    "response = chat_with_bot(user_input)\n",
    "print(f\"You: {user_input}\")\n",
    "print(f\"\\nChatbot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 3\n",
    "user_input = \"What temperature should I bake my sourdough at?\"\n",
    "response = chat_with_bot(user_input)\n",
    "print(f\"You: {user_input}\")\n",
    "print(f\"\\nChatbot: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Chat Loop\n",
    "\n",
    "Run this cell to have an interactive conversation with the chatbot. Type 'exit' or 'quit' to end the conversation.\n",
    "\n",
    "**Note:** This will only work in a regular Jupyter notebook, not in JupyterLab or some cloud environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chat loop\n",
    "print(\"Chatbot: Hello! I'm your sourdough bread assistant. Ask me anything about sourdough baking!\")\n",
    "print(\"(Type 'exit' or 'quit' to end the conversation)\\n\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"\\nChatbot: Happy baking!\")\n",
    "            break\n",
    "        \n",
    "        response = chat_with_bot(user_input)\n",
    "        print(f\"\\nChatbot: {response}\\n\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nChatbot: Happy baking!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Conversation History\n",
    "\n",
    "The above chatbot doesn't remember previous messages. Here's an improved version that maintains conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    \"\"\"\n",
    "    A chatbot with conversation memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, system_prompt=None, model=\"llama-3.3-70b-versatile\"):\n",
    "        self.model = model\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        # Set system prompt\n",
    "        if system_prompt is None:\n",
    "            system_prompt = \"You are an expert baker assistant that helps home bakers with questions about sourdough bread dough.\"\n",
    "        \n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        })\n",
    "    \n",
    "    def chat(self, user_message):\n",
    "        \"\"\"\n",
    "        Send a message and get a response while maintaining conversation history.\n",
    "        \"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            # Get response from Groq\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=self.conversation_history,\n",
    "                temperature=0.7,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            \n",
    "            # Extract assistant's message\n",
    "            assistant_message = response.choices[0].message.content\n",
    "            \n",
    "            # Add assistant's response to history\n",
    "            self.conversation_history.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": assistant_message\n",
    "            })\n",
    "            \n",
    "            return assistant_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset conversation history.\"\"\"\n",
    "        system_message = self.conversation_history[0]\n",
    "        self.conversation_history = [system_message]\n",
    "        print(\"Conversation history reset!\")\n",
    "\n",
    "print(\"✅ Chatbot class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chatbot with memory\n",
    "bot = Chatbot()\n",
    "\n",
    "# First question\n",
    "response1 = bot.chat(\"What's the ideal temperature for proofing sourdough?\")\n",
    "print(\"You: What's the ideal temperature for proofing sourdough?\")\n",
    "print(f\"Bot: {response1}\\n\")\n",
    "\n",
    "# Follow-up question (references previous context)\n",
    "response2 = bot.chat(\"What if my kitchen is colder than that?\")\n",
    "print(\"You: What if my kitchen is colder than that?\")\n",
    "print(f\"Bot: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see the full conversation history\n",
    "print(\"Full conversation history:\")\n",
    "for msg in bot.conversation_history:\n",
    "    role = msg['role'].capitalize()\n",
    "    content = msg['content'][:100] + \"...\" if len(msg['content']) > 100 else msg['content']\n",
    "    print(f\"\\n{role}: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-workshop",
   "language": "python",
   "name": "genai-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
