{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a9aa5d",
   "metadata": {
    "id": "a7a9aa5d"
   },
   "source": [
    "# Demo 1 - Customizing Groq LLMs for Diverse Language Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ab3c4-df7b-46a6-be0c-a70a45b3067d",
   "metadata": {
    "id": "6c7ab3c4-df7b-46a6-be0c-a70a45b3067d"
   },
   "source": [
    "## Introduction\n",
    "This lab focuses on demonstrating prompt engineering techniques using Groq. Prompt engineering is a critical aspect of utilizing Groq models effectively, as it involves crafting prompts that generate desired outputs. In this lab, we will cover various scenarios, including QnA, summarizing text, classifying text, generating new product names, translation, parsing unstructured data, and translating natural language queries into SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2160dc83-b832-470b-bbe5-0813e36636a5",
   "metadata": {
    "id": "2160dc83-b832-470b-bbe5-0813e36636a5"
   },
   "source": [
    "## Prerequisites\n",
    " - Ensure you have a Groq account.\n",
    " - Install the required libraries: `groq` and `python-dotenv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb96b08",
   "metadata": {
    "id": "4bb96b08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from groq) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/mannasethsotsky/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install groq python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d61902",
   "metadata": {
    "id": "e3d61902",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import groq\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pJY3o96aIWca",
   "metadata": {
    "id": "pJY3o96aIWca"
   },
   "outputs": [],
   "source": [
    "# Load the environment variables (if set manually)\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Configure Groq client\n",
    "client = Groq(api_key=api_key)\n",
    "model = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "def chat(prompt, temperature=0.7, max_tokens=50, system=\"You are a helpful assistant.\"):\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e727e9-6f53-4b56-83f9-605ba2ea96e9",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "1.  **Prompt Definition:** The variable `prompt` contains the question provided to the model. In this case, the question is \"Who are you?\"\n",
    "    \n",
    "2.  **API Call:** The `client.chat.completions.create` function is used to send a request to the Groq LLMs API. It takes parameters such as the engine (model), the prompt (question), temperature, and max_tokens (maximum number of tokens in the response).\n",
    "    \n",
    "3.  **Response Handling:** The API response is stored in the `results` variable, and the generated text is extracted from it using `results[\"choices\"][0][\"text\"]`. The `strip(\"\\n\")` function is used to remove leading and trailing newline characters.\n",
    "    \n",
    "4.  **Output Printing:** The final generated text is printed to the console using `print(results[\"choices\"][0][\"text\"].strip(\"\\n\"))`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ac1fa",
   "metadata": {
    "id": "228ac1fa"
   },
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a33a2",
   "metadata": {
    "id": "002a33a2"
   },
   "source": [
    "### 1. QnA\n",
    "In this section, we will use Groq to answer questions.\n",
    "\n",
    "The primary goal of this section is to showcase how the model can understand and generate meaningful responses based on the given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8V-pTJeXl22D",
   "metadata": {
    "id": "8V-pTJeXl22D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence (AI) assistant, which means I'm a computer program designed to help and communicate with humans. My purpose is to provide information, answer questions, and assist with tasks to the best of my abilities. I don't have\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Who are you?\n",
    "\"\"\"\n",
    "\n",
    "results = chat(\n",
    "    prompt,\n",
    "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
    "    max_tokens=50  # Increase max_tokens for longer responses\n",
    ")\n",
    "\n",
    "output_text = results.choices[0].message.content.strip(\"\\n\")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448867c5-e9e7-4300-a55c-17545692192d",
   "metadata": {
    "id": "448867c5-e9e7-4300-a55c-17545692192d"
   },
   "source": [
    "\n",
    "    \n",
    "\n",
    "### Purpose\n",
    "\n",
    "-   **Demonstrating Question Understanding:** This section serves to illustrate how the Groq LLMs model can comprehend and respond appropriately to a given question. The model attempts to provide a coherent answer based on its understanding of the input prompt.\n",
    "    \n",
    "-   **Interaction with the Model:** It showcases the basic interaction pattern with the Groq API for question and answer tasks, emphasizing the simplicity of the API integration for such scenarios.\n",
    "    \n",
    "-   **User-Specific Applications:** This functionality is valuable for a range of applications, including chatbots, virtual assistants, and information retrieval systems where users can pose questions, and the system generates relevant responses.\n",
    "    \n",
    "-   **Flexibility in Questioning:** The model's ability to handle various types of questions and provide contextually relevant answers highlights its versatility in natural language understanding.\n",
    "    \n",
    "    \n",
    "\n",
    "> ### Note\n",
    ">\n",
    "> Depending on the nature of the prompt and the desired application, additional parameters such as temperature (controls randomness in the model's output) and max_tokens (limits the length of the response) can be adjusted to fine-tune the behavior of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb89a8de-f3c2-4f97-a5d6-3f5e58f201ba",
   "metadata": {
    "id": "fb89a8de-f3c2-4f97-a5d6-3f5e58f201ba",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```yml\n",
      "product:\n",
      "  name: iPhone 13\n",
      "  price: $799\n",
      "  availability: In stock\n",
      "  features:\n",
      "    - 5G\n",
      "    - OLED display\n",
      "    - A15 chip\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Generate a YAML response for a product:\n",
    "- Product Name: iPhone 13\n",
    "- Price: $799\n",
    "- Availability: In stock\n",
    "- Features: 5G, OLED display, A15 chip\n",
    "\"\"\"\n",
    "\n",
    "results = chat(\n",
    "    prompt,\n",
    "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
    "    max_tokens=50  # Increase max_tokens for longer responses\n",
    ")\n",
    "\n",
    "output_text = results.choices[0].message.content.strip(\"\\n\")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a70a9-5863-4650-a516-ed61df4e3496",
   "metadata": {
    "id": "824a70a9-5863-4650-a516-ed61df4e3496"
   },
   "source": [
    "Generate text in French based on a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd361b3c-e1b7-4288-a067-f16a637f42b8",
   "metadata": {
    "id": "fd361b3c-e1b7-4288-a067-f16a637f42b8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour! Comment allez-vous aujourd'hui ? (Hello! How are you today?)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Bonjour\"\"\"\n",
    "\n",
    "results = chat(\n",
    "    prompt,\n",
    "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
    "    max_tokens=50  # Increase max_tokens for longer responses\n",
    ")\n",
    "\n",
    "output_text = results.choices[0].message.content.strip(\"\\n\")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ca234-4f05-461b-9b68-09ed8c78fc75",
   "metadata": {
    "id": "bd8ca234-4f05-461b-9b68-09ed8c78fc75"
   },
   "source": [
    "### Explanation\n",
    "\n",
    "1.  **Prompt Definition:** The variable `prompt` contains a French greeting, \"Bonjour,\" which translates to \"Hello\" in English.\n",
    "    \n",
    "2.  **API Call:** The `client.chat.completions.create` function is used to send a request to the Groq LLMs API. The parameters include:\n",
    "    \n",
    "    -   `engine`: Specifies the model to use (`text-davinci-003` in this case).\n",
    "    -   `prompt`: The input text provided to the model (`\"Bonjour\"` in French).\n",
    "    -   `temperature`: Controls the randomness of the model's output (set to `0` for deterministic responses).\n",
    "    -   `max_tokens`: Limits the length of the generated text to 800 tokens.\n",
    "3.  **Response Handling:** The API response is stored in the `results` variable. The generated text is then extracted using `results[\"choices\"][0][\"text\"]`. The `strip(\"\\n\")` function is used to remove leading and trailing newline characters.\n",
    "    \n",
    "4.  **Output Printing:** The final generated text is printed to the console using `print(results[\"choices\"][0][\"text\"].strip(\"\\n\"))`.\n",
    "    \n",
    "\n",
    "### Purpose\n",
    "\n",
    "-   **Language Translation:** This code snippet demonstrates the model's ability to understand and respond in French. It can be used for language translation tasks where the model takes input in one language and generates corresponding output in another language.\n",
    "    \n",
    "-   **Multilingual Capabilities:** GPT-3.5's multilingual capabilities allow it to handle prompts in various languages, showcasing its versatility in natural language understanding and generation.\n",
    "    \n",
    "-   **User Interface Localization:** In applications with multilingual user interfaces, this capability can be employed to dynamically generate responses in the user's preferred language.\n",
    "    \n",
    "\n",
    "> ### Note\n",
    ">\n",
    "> -   The choice of the `temperature` parameter influences the randomness of the model's responses. A lower temperature value (e.g., 0) produces more deterministic and focused output, while higher values  introduce more randomness.\n",
    ">\n",
    "> -   Adjustments to the `max_tokens` parameter can be made based on the desired length of the generated text. Setting an appropriate value prevents overly long responses.\n",
    ">     \n",
    "> -   This code illustrates how GPT-3.5 can seamlessly handle prompts in different languages, showcasing its potential in internationalization and localization contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399df257",
   "metadata": {
    "id": "399df257",
    "tags": []
   },
   "source": [
    "### 2. Summarize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60206f2a-d80c-4cf7-a573-a937469b860b",
   "metadata": {
    "id": "60206f2a-d80c-4cf7-a573-a937469b860b"
   },
   "source": [
    "Model's ability to summarize a given text into three short bullet points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77a97dbc-8703-4d87-821a-348d2e47aadb",
   "metadata": {
    "id": "77a97dbc-8703-4d87-821a-348d2e47aadb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 3 short bullet points summarizing the text:\n",
      "\n",
      "* A neutron star is formed from the collapsed core of a massive supergiant star.\n",
      "* Neutron stars are extremely dense, with a radius of about 10 km and a mass of 1.4 solar masses.\n",
      "* They are formed through a supernova explosion and gravitational collapse of a massive star's core.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Summarize below text in 3 short bullet points：\n",
    "\n",
    "            ###\n",
    "            A neutron star is the collapsed core of a massive supergiant star,\n",
    "            which had a total mass of between 10 and 25 solar masses,\n",
    "            possibly more if the star was especially metal-rich.\n",
    "            Neutron stars are the smallest and densest stellar objects,\n",
    "            excluding black holes and hypothetical white holes, quark stars,\n",
    "            and strange stars. Neutron stars have a radius on the order of\n",
    "            10 kilometres (6.2 mi) and a mass of about 1.4 solar masses.\n",
    "            They result from the supernova explosion of a massive star,\n",
    "            combined with gravitational collapse, that compresses the core\n",
    "            past white dwarf star density to that of atomic nuclei.\n",
    "            ###\n",
    "         \"\"\"\n",
    "\n",
    "results = chat(\n",
    "    prompt,\n",
    "    system=\"You are an astrolger assistant.\",\n",
    "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
    "    max_tokens=800  # Increase max_tokens for longer responses\n",
    ")\n",
    "\n",
    "output_text = results.choices[0].message.content.strip(\"\\n\")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24743ef3-5bd0-4e5a-894a-c56525661e83",
   "metadata": {
    "id": "24743ef3-5bd0-4e5a-894a-c56525661e83"
   },
   "source": [
    "### Explanation\n",
    "\n",
    "1.  **Prompt Definition:**\n",
    "    \n",
    "    -   The variable `prompt` contains a lengthy text describing neutron stars. The prompt instructs the model to summarize this text into three short bullet points.\n",
    "2.  **API Call:**\n",
    "    \n",
    "    -   The `client.chat.completions.create` function is used to send a request to the Groq LLMs API.\n",
    "    -   The `engine` parameter specifies the GPT-3.5 engine or model to be used.\n",
    "    -   The `prompt` parameter provides the input text (description of a neutron star) to the model.\n",
    "    -   The `temperature` parameter is set to 0, indicating that the model's output should be deterministic and focused.\n",
    "    -   The `max_tokens` parameter limits the length of the generated text to 800 tokens.\n",
    "3.  **Response Handling:**\n",
    "    \n",
    "    -   The API response, including the generated text, is stored in the `results` variable.\n",
    "4.  **Output Printing:**\n",
    "    \n",
    "    -   The generated summary text is extracted from the API response using `results[\"choices\"][0][\"text\"].strip(\"\\n\")`.\n",
    "    -   The `strip(\"\\n\")` function removes leading and trailing newline characters.\n",
    "    -   The final result is printed to the console.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "-   **Text Summarization:**\n",
    "    \n",
    "    -   The primary purpose is to showcase the model's ability to condense a longer piece of text into a concise summary, represented as three short bullet points.\n",
    "-   **Information Extraction:**\n",
    "    \n",
    "    -   It demonstrates how the model can extract key information from a given passage and present it in a structured format, which is particularly useful for distilling essential details from large bodies of text.\n",
    "-   **Automation of Summary Generation:**\n",
    "    \n",
    "    -   Developers can use this code as a foundation for automating the process of summarizing documents or articles, streamlining information extraction tasks.\n",
    "-   **Scalable Content Processing:**\n",
    "    \n",
    "    -   This functionality is valuable in scenarios where there is a need to process and summarize large volumes of textual information efficiently.\n",
    "\n",
    "> ### Note\n",
    ">\n",
    "> -   Developers can experiment with different prompts, text lengths, and temperature settings to fine-tune the summarization process based on their specific use case requirements.\n",
    ">     \n",
    "> -   This code snippet illustrates how the Groq LLMs model can be leveraged for content summarization, offering a glimpse into its capabilities in handling natural language understanding and generation  tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988edc7-b4c8-47aa-a9ad-b9fdc7268235",
   "metadata": {
    "id": "1988edc7-b4c8-47aa-a9ad-b9fdc7268235"
   },
   "source": [
    "### Chinese Text Summarization\n",
    "\n",
    "The provided code snippet focuses on leveraging the Groq LLMs model to generate a brief summary of a Chinese text. The text in question describes the characteristics of a neutron star, including its mass, size, and the process by which it is formed. The goal is to obtain a concise summary represented in simplified Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e5c1b8b-ad07-4e17-af99-65c01baf6bc5",
   "metadata": {
    "id": "8e5c1b8b-ad07-4e17-af99-65c01baf6bc5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a brief summary of the text in English:\n",
      "\n",
      "A neutron star is the collapsed core of a massive star (10-25 times the mass of the sun) that has undergone a supernova explosion and gravitational collapse. It is one of the smallest and densest stellar objects, with a radius of about 10 kilometers and a mass of approximately 1.4 times that of the sun.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"简要概括下面文字：\n",
    "\n",
    "            ###\n",
    "            中子星是一颗质量达10至25太阳质量（如果恒星特别富含金属可能更多）的超级巨星的坍缩核心。\n",
    "            中子星是最小最密集的恒星物体，除了黑洞和假想的白洞、夸克星和奇异星。\n",
    "            中子星的半径约为10公里（6.2英里），质量约为1.4太阳质量。\n",
    "            它们是由超级新星爆炸和引力坍缩共同产生的，使核心压缩到白矮星密度以上的原子核密度。\n",
    "            ###\n",
    "\n",
    "\n",
    "         \"\"\"\n",
    "\n",
    "results = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": \"convert to english\"}\n",
    "    ],\n",
    "    temperature=0,  # You can adjust the temperature for more creative or focused outputs\n",
    "    max_tokens=800  # Increase max_tokens for longer responses\n",
    ")\n",
    "\n",
    "output_text = results.choices[0].message.content.strip(\"\\n\")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b37557-9e52-494b-b4eb-968f478fd1ad",
   "metadata": {
    "id": "d2b37557-9e52-494b-b4eb-968f478fd1ad"
   },
   "source": [
    "\n",
    "#### Explanation\n",
    "\n",
    "1.  **Prompt Definition:**\n",
    "    \n",
    "    -   The variable `prompt` contains a Chinese passage instructing the model to summarize the provided text.\n",
    "2.  **API Call:**\n",
    "    \n",
    "    -   The `client.chat.completions.create` function is used to send a request to the Groq LLMs API.\n",
    "    -   The `engine` parameter specifies the GPT-3.5 engine or model to be used.\n",
    "    -   The `prompt` parameter provides the input text (description of a neutron star in Chinese) to the model.\n",
    "    -   The `temperature` parameter is set to 0, indicating that the model's output should be deterministic and focused.\n",
    "    -   The `max_tokens` parameter limits the length of the generated text to 800 tokens.\n",
    "3.  **Response Handling:**\n",
    "    \n",
    "    -   The API response, including the generated summary text, is stored in the `results` variable.\n",
    "4.  **Output Printing:**\n",
    "    \n",
    "    -   The generated summary text is extracted from the API response using `results[\"choices\"][0][\"text\"].strip(\"\\n\")`.\n",
    "    -   The `strip(\"\\n\")` function removes leading and trailing newline characters.\n",
    "    -   The final result is printed to the console.\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "-   **Cross-Language Summarization:**\n",
    "    \n",
    "    -   This code exemplifies the Groq LLMs model's capability to summarize information in a language other than English, showcasing its multilingual text processing capabilities.\n",
    "-   **Automated Content Summarization:**\n",
    "    \n",
    "    -   It illustrates how the model can be used to automate the process of summarizing content in Chinese, providing a concise overview of a given text.\n",
    "-   **Language Understanding and Generation:**\n",
    "    \n",
    "    -   The code demonstrates the model's proficiency in understanding and generating text in a non-English language, extending its utility to a global context.\n",
    "\n",
    "#### Note\n",
    "\n",
    "-   Developers can adapt this code for various applications, such as building multilingual summarization tools or integrating cross-language capabilities into their natural language processing workflows.\n",
    "    \n",
    "-   Adjustments to the prompt, temperature, and other parameters can be made based on specific use case requirements and language nuances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7814896f",
   "metadata": {
    "id": "7814896f"
   },
   "source": [
    "### 3. Classify Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f567f-ac44-4cec-bbdb-585522b770d8",
   "metadata": {
    "id": "f79f567f-ac44-4cec-bbdb-585522b770d8"
   },
   "source": [
    "`Classify Text` refers to the task of assigning predefined categories or labels to a given piece of text based on its content and context. The goal is to automatically categorize the text into one or more predefined classes, making it easier to organize, search, and analyze large volumes of textual data. This task falls under the broader field of natural language processing (NLP) and is commonly used in various applications, including sentiment analysis, topic categorization, and content filtering.\n",
    "\n",
    "The process typically involves training a machine learning model on a labeled dataset where each text sample is associated with its corresponding category. The model learns patterns and features from the training data, allowing it to generalize and classify new, unseen text accurately.\n",
    "\n",
    "In the context of the provided code snippet, \"Classify Text\" specifically refers to instructing the Groq LLMs model to categorize a news article into one of the specified categories: Tech, Politics, Sport, or Entertainment. The model generates a response that represents its prediction of the most suitable category based on its understanding of the given news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bcd125e-407b-4d97-bd92-e4e8dc3d4e5d",
   "metadata": {
    "id": "9bcd125e-407b-4d97-bd92-e4e8dc3d4e5d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the content of the news article, I would classify the first part as \"Entertainment\" since it talks about a cooking guru and a new book, which suggests a focus on lifestyle or celebrity culture.\n",
      "\n",
      "The second part would be classified as \"Sport\" since it describes a scene of competition, with a scoreboard, a final buzzer, and a team celebrating a victory, which are all typical elements of a sports event.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Classify the following news article into 1 of the following categories:\n",
    "            [Tech, Politics, Sport, Entertainment]\n",
    "\n",
    "            ###\n",
    "            Donna Steffensen Is Cooking Up a New Kind of Perfection.\n",
    "            The Internet’s most beloved cooking guru has a buzzy new book and\n",
    "            a fresh new perspective: Entertainment\n",
    "\n",
    "            Donna Steffensen Is Cooking Up a New Kind of Perfection.\n",
    "            The Internet’s most beloved cooking guru has a buzzy new book and\n",
    "            a fresh new perspective: Politics\n",
    "\n",
    "            Maya's heart pounded as the final buzzer blared.\n",
    "            Sweat stung her eyes, but a smile stretched across her face.\n",
    "            The scoreboard confirmed it: victory by one point.\n",
    "            Exhausted but elated, Maya high-fived her teammates,\n",
    "            the cheers of the crowd a sweet reward.:\n",
    "\n",
    "            ###\n",
    "         \"\"\"\n",
    "\n",
    "results = chat(\n",
    "    prompt,\n",
    "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
    "    max_tokens=800  # Increase max_tokens for longer responses\n",
    ")\n",
    "\n",
    "output_text = results.choices[0].message.content.strip(\"\\n\")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3M6bvFXomL5L",
   "metadata": {
    "id": "3M6bvFXomL5L"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-cb2ec0c9-2bf4-41e9-be42-dba41e47ffb7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Based on the content of the news article, I would classify the first part as \"Entertainment\" since it talks about a cooking guru and a new book, which suggests a focus on lifestyle or celebrity culture.\\n\\nThe second part would be classified as \"Sport\" since it describes a scene of competition, with a scoreboard, a final buzzer, and a team celebrating a victory, which are all typical elements of a sports event.', role='assistant', annotations=None, executed_tools=None, function_call=None, reasoning=None, tool_calls=None))], created=1768854926, model='llama-3.3-70b-versatile', object='chat.completion', mcp_list_tools=None, service_tier='on_demand', system_fingerprint='fp_f8b414701e', usage=CompletionUsage(completion_tokens=87, prompt_tokens=212, total_tokens=299, completion_time=0.350059523, completion_tokens_details=None, prompt_time=0.028303028, prompt_tokens_details=None, queue_time=0.192210727, total_time=0.378362551), usage_breakdown=None, x_groq=XGroq(id='req_01kfbzgcn4ef9b9ay4n85qc7ky', debug=None, seed=906379197, usage=None))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6109264-5d7f-4e93-acb8-365e4852e38e",
   "metadata": {
    "id": "e6109264-5d7f-4e93-acb8-365e4852e38e"
   },
   "source": [
    "#### Description\n",
    "\n",
    "1.  **Prompt Definition:**\n",
    "    \n",
    "    -   The `prompt` variable sets the instructions for the model. It instructs the model to classify the provided news article into one of the predefined categories: Tech, Politics, Sport, or Entertainment.\n",
    "2.  **News Article:**\n",
    "    \n",
    "    -   The provided news article is about Donna Steffensen, described as a cooking guru with a new book and a fresh perspective.\n",
    "3.  **API Call:**\n",
    "    \n",
    "    -   The `client.chat.completions.create` function sends a request to the Groq LLMs API for text generation.\n",
    "    -   The `engine` parameter specifies the GPT-3.5 engine or model to be used.\n",
    "    -   The `prompt` parameter supplies the input text (news article and classification instruction) to the model.\n",
    "    -   The `temperature` parameter is set to 0, indicating a deterministic response without randomness.\n",
    "    -   The `max_tokens` parameter limits the length of the generated text to 800 tokens.\n",
    "4.  **Response Handling:**\n",
    "    \n",
    "    -   The API response, including the generated text (classification result), is stored in the `results` variable.\n",
    "5.  **Output Printing:**\n",
    "    \n",
    "    -   The generated classification result is extracted from the API response using `results[\"choices\"][0][\"text\"].strip(\"\\n\")`.\n",
    "    -   The `strip(\"\\n\")` function removes leading and trailing newline characters.\n",
    "    -   The final result is printed to the console.\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "-   **Text Classification:**\n",
    "    \n",
    "    -   The code demonstrates the use of the Groq LLMs model for text classification. The model categorizes a given news article into one of the specified classes (Tech, Politics, Sport, or Entertainment).\n",
    "-   **Automated Categorization:**\n",
    "    \n",
    "    -   It showcases the potential for automating the categorization of news articles based on their content, providing a quick and efficient way to organize information.\n",
    "-   **Understanding Context:**\n",
    "    \n",
    "    -   The model's ability to comprehend the context of the news article and assign it to a relevant category highlights its proficiency in understanding natural language and context.\n",
    "\n",
    "#### Note\n",
    "\n",
    "-   Developers can customize the prompt and categories based on their specific use case, expanding the application to various domains that involve text classification.\n",
    "    \n",
    "-   Fine-tuning the parameters, such as temperature, can be explored to influence the randomness and creativity of the generated classification result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127f95b",
   "metadata": {
    "id": "7127f95b"
   },
   "source": [
    "### 4. Generate New Product Name\n",
    "\n",
    "It automatically create a unique product name based on specified information. With a product description of a \"home milkshake maker\" and seed words like \"fast, healthy, compact,\" the model generates a fitting name inspired by provided examples. This showcases the capability of the model in creative product naming, offering a streamlined and automated approach for inventing distinctive names for various products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68e7324a-e559-42f6-90ce-58f98a92a8b2",
   "metadata": {
    "id": "68e7324a-e559-42f6-90ce-58f98a92a8b2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the product description and seed words, I'd like to suggest some new product name ideas:\n",
      "\n",
      "1. **NutriBlend**: Emphasizes the healthy aspect of the product, suggesting a blender that creates nutritious milkshakes.\n",
      "2. **CompactCream**: Highlights the compact design of the product while evoking the creamy texture of milkshakes.\n",
      "3. **FastFusion**: Combines the idea of speed with the blending process, implying a quick and efficient milkshake maker.\n",
      "4. **SlimShake**: Plays off the compact seed word, positioning the product as a sleek and slim solution for home milkshake creation.\n",
      "5. **HealthyHurricane**: Uses a playful, whimsical name to convey the idea of a powerful, healthy milkshake maker that's easy to use.\n",
      "6. **QuickSip**: Emphasizes the speed and convenience of the product, suggesting a fast and easy way to enjoy a milkshake at home.\n",
      "7. **PureShake**: Combines the ideas of healthy and compact, implying a product that creates pure, unadulterated milkshakes with minimal fuss.\n",
      "\n",
      "These names are designed to be catchy, memorable, and easy to spell, while also conveying the key benefits of the product. Let me know if you'd like me to brainstorm more ideas or if you have any specific preferences!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Generate new product name based on the following information：\n",
    "\n",
    "            ###\n",
    "            Product description: A home milkshake maker\n",
    "            Seed words: fast, healthy, compact\n",
    "            Product names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "            ###\n",
    "         \"\"\"\n",
    "\n",
    "results = chat(\n",
    "    prompt,\n",
    "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
    "    max_tokens=800  # Increase max_tokens for longer responses\n",
    ")\n",
    "\n",
    "output_text = results.choices[0].message.content.strip(\"\\n\")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40e666",
   "metadata": {
    "id": "4b40e666"
   },
   "source": [
    "### 5. Translation\n",
    "\n",
    "In \"Translation\" you translate a Chinese poem into English. The provided poem describes the scenery of mountains, the flow of the Yellow River into the sea, and the desire to explore distant landscapes. The model generates an English translation, showcasing its ability to understand and convert text between different languages. The resulting translation is then printed to the console, demonstrating the model's proficiency in cross-language tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df4f0220",
   "metadata": {
    "id": "df4f0220",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the translation of the poem:\n",
      "\n",
      "            ###\n",
      "            The white sun sets behind the mountains, the Yellow River flows into the sea.\n",
      "            To gaze at a thousand miles, climb up another story.\n",
      "            ###\n",
      "\n",
      "Or, in a more poetic translation:\n",
      "\n",
      "            ###\n",
      "            As sunset fades beyond the hills, the Yellow River flows to the sea.\n",
      "            To see a thousand miles, ascend one more floor.\n",
      "            ###\n",
      "\n",
      "This poem is a famous Chinese poem by Wang Zhihuan, a poet of the Tang Dynasty. The poem is about the pursuit of knowledge and the desire to see beyond the horizon. The last line, \"欲穷千里目，更上一层楼\" (To gaze at a thousand miles, climb up another story), has become a Chinese idiom, meaning to strive for greater heights and broader perspectives.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"请用英语翻译下面这首诗歌：\n",
    "\n",
    "            ###\n",
    "            白日依山尽，黄河入海流。\n",
    "            欲穷千里目，更上一层楼。\n",
    "            ###\n",
    "         \"\"\"\n",
    "\n",
    "results = chat(\n",
    "    prompt,\n",
    "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
    "    max_tokens=800  # Increase max_tokens for longer responses\n",
    ")\n",
    "\n",
    "output_text = results.choices[0].message.content.strip(\"\\n\")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783da05",
   "metadata": {
    "id": "d783da05"
   },
   "source": [
    "### 6. Parse Unstructured Data\n",
    "\n",
    "In \"Parse Unstructured Data\" you process information about fruits on the fictional planet Goocrux. Descriptions of various fruits, such as neoskizzles, loheckles, pounits, loopnovas, and glowls, are provided. The code instructs the model to create a structured table summarizing these fruits, including details like color and flavor. The resulting table is generated by the model and printed to the console, showcasing the model's ability to organize and present information in a tabular format based on unstructured input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d38ed82e",
   "metadata": {
    "id": "d38ed82e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a JSON object summarizing the fruits from Goocrux:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Fruits\": [\n",
      "    {\n",
      "      \"Name\": \"Neoskizzle\",\n",
      "      \"Color\": \"Purple\",\n",
      "      \"Flavor\": \"Candy-like\"\n",
      "    },\n",
      "    {\n",
      "      \"Name\": \"Loheckle\",\n",
      "      \"Color\": \"Grayish Blue\",\n",
      "      \"Flavor\": \"Tart, similar to lemon\"\n",
      "    },\n",
      "    {\n",
      "      \"Name\": \"Pounit\",\n",
      "      \"Color\": \"Bright Green\",\n",
      "      \"Flavor\": \"Savory\"\n",
      "    },\n",
      "    {\n",
      "      \"Name\": \"Loopnova\",\n",
      "      \"Color\": \"Neon Pink\",\n",
      "      \"Flavor\": \"Cotton candy-like\"\n",
      "    },\n",
      "    {\n",
      "      \"Name\": \"Glowl\",\n",
      "      \"Color\": \"Pale Orange\",\n",
      "      \"Flavor\": \"Sour and bitter, acidic and caustic\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "Alternatively, if you'd like a simpler JSON structure, you can use the following:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Neoskizzle\": {\"Color\": \"Purple\", \"Flavor\": \"Candy-like\"},\n",
      "  \"Loheckle\": {\"Color\": \"Grayish Blue\", \"Flavor\": \"Tart, similar to lemon\"},\n",
      "  \"Pounit\": {\"Color\": \"Bright Green\", \"Flavor\": \"Savory\"},\n",
      "  \"Loopnova\": {\"Color\": \"Neon Pink\", \"Flavor\": \"Cotton candy-like\"},\n",
      "  \"Glowl\": {\"Color\": \"Pale Orange\", \"Flavor\": \"Sour and bitter, acidic and caustic\"}\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"There are many fruits that were found on the recently discovered planet Goocrux.\n",
    "            There are neoskizzles that grow there, which are purple and taste like candy.\n",
    "            There are also loheckles, which are a grayish blue fruit and are very tart, a\n",
    "            little bit like a lemon. Pounits are a bright green color and are more savory\n",
    "            than sweet. There are also plenty of loopnovas which are a neon pink flavor and\n",
    "            taste like cotton candy. Finally, there are fruits called glowls, which have a very\n",
    "            sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\n",
    "\n",
    "            ###\n",
    "            Please make a json summarizing the fruits from Goocrux\n",
    "            { \"Fruit\":\n",
    "              \"Color\":\n",
    "              \"Flavor\":}\n",
    "            ###\n",
    "         \"\"\"\n",
    "\n",
    "results = chat(\n",
    "    prompt,\n",
    "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
    "    max_tokens=800  # Increase max_tokens for longer responses\n",
    ")\n",
    "\n",
    "output_text = results.choices[0].message.content.strip(\"\\n\")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b8a78",
   "metadata": {
    "id": "035b8a78"
   },
   "source": [
    "### 7. NLP to SQL\n",
    "\n",
    "In \"NLP to SQL\" you convert natural language instructions into a SQL query. The provided prompt defines three PostgreSQL tables (Employee, Department, and Salary_Payments) with their respective properties. The code then instructs the model to generate a SQL query for listing the names of departments that employed more than 10 employees in the last 3 months.\n",
    "\n",
    "The resulting SQL query is generated by the model and printed to the console, showcasing the model's ability to understand and convert natural language queries into structured SQL statements for database operations. This code provides an example of how natural language processing can be applied to automate the generation of SQL queries based on human-readable instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80feffef",
   "metadata": {
    "id": "80feffef",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve this problem, we need to write a SQL query that joins the `Employee`, `Department`, and `Salary_Payments` tables based on their respective keys. We then filter the results to only include departments that had more than 10 employees with salary payments in the last 3 months.\n",
      "\n",
      "Here is the SQL query:\n",
      "```sql\n",
      "SELECT D.name\n",
      "FROM Department D\n",
      "WHERE D.id IN (\n",
      "    SELECT E.department_id\n",
      "    FROM Employee E\n",
      "    JOIN Salary_Payments SP ON E.id = SP.employee_id\n",
      "    WHERE SP.date > NOW() - INTERVAL '3 months'\n",
      "    GROUP BY E.department_id\n",
      "    HAVING COUNT(DISTINCT E.id) > 10\n",
      ");\n",
      "```\n",
      "Let's break down this query:\n",
      "\n",
      "1. We join the `Employee` and `Salary_Payments` tables on the `employee_id` column.\n",
      "2. We filter the results to only include rows where the `date` column in the `Salary_Payments` table is within the last 3 months.\n",
      "3. We group the results by the `department_id` column in the `Employee` table.\n",
      "4. We use the `HAVING` clause to filter the results to only include groups with more than 10 distinct employees.\n",
      "5. Finally, we select the `name` column from the `Department` table for the departments that meet the conditions.\n",
      "\n",
      "This query will return the names of the departments that employed more than 10 employees in the last 3 months.\n",
      "\n",
      "Example use case:\n",
      "\n",
      "Suppose we have the following data:\n",
      "```sql\n",
      "-- Employee table\n",
      "+----+--------+---------------+\n",
      "| id | name   | department_id |\n",
      "+----+--------+---------------+\n",
      "| 1  | John   | 1             |\n",
      "| 2  | Jane   | 1             |\n",
      "| 3  | Bob    | 2             |\n",
      "| ... | ...    | ...           |\n",
      "+----+--------+---------------+\n",
      "\n",
      "-- Salary_Payments table\n",
      "+----+------------+--------+------------+\n",
      "| id | employee_id | amount | date       |\n",
      "+----+------------+--------+------------+\n",
      "| 1  | 1          | 1000   | 2022-09-15 |\n",
      "| 2  | 2          | 1200   | 2022-10-01 |\n",
      "| 3  | 3          | 1500   | 2022-08-20 |\n",
      "| ... | ...        | ...    | ...        |\n",
      "+----+------------+--------+------------+\n",
      "\n",
      "-- Department table\n",
      "+----+----------+---------+\n",
      "| id | name     | address |\n",
      "+----+----------+---------+\n",
      "| 1  | Sales    | NYC     |\n",
      "| 2  | Marketing| LA      |\n",
      "| ... | ...      | ...     |\n",
      "+----+----------+---------+\n",
      "```\n",
      "If we run the query on this data, we might get the following result:\n",
      "```sql\n",
      "+----------+\n",
      "| name     |\n",
      "+----------+\n",
      "| Sales    |\n",
      "+----------+\n",
      "```\n",
      "This means that the Sales department employed more than 10 employees in the last 3 months.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"### Postgres SQL tables, with their properties:\n",
    "            #\n",
    "            # Employee(id, name, department_id)\n",
    "            # Department(id, name, address)\n",
    "            # Salary_Payments(id, employee_id, amount, date)\n",
    "            #\n",
    "\n",
    "            ### A query to list the names of the departments\n",
    "                which employed more than 10 employees in the last 3 months\n",
    "            ###\n",
    "         \"\"\"\n",
    "\n",
    "results = chat(\n",
    "    prompt,\n",
    "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
    "    max_tokens=800  # Increase max_tokens for longer responses\n",
    ")\n",
    "\n",
    "output_text = results.choices[0].message.content.strip(\"\\n\")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20daa8",
   "metadata": {
    "id": "df20daa8"
   },
   "outputs": [],
   "source": [
    "# @title Default title text\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "genai-workshop-docusign",
   "language": "python",
   "name": "genai-workshop-docusign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
